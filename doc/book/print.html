<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LLM-Hosting</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-371823cd.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-29b41a76.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">LLM-Hosting</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="einleitung-überblick-und-definitionen"><a class="header" href="#einleitung-überblick-und-definitionen">Einleitung, Überblick und Definitionen</a></h1>
<p>Dieses Material entstand auf der Grundlage eines Vortrags<sup class="footnote-reference" id="fr-1-1"><a href="#footnote-1">1</a></sup> über Self-Hosting von LLMs, der auf dem <a href="https://www.youtube.com/watch?v=exOeJCLcuHQ">Datenfestival</a> des <a href="https://civic-data.de/">Civic Data Lab</a> gehalten wurde. Es soll den Vortrag zugänglicher machen, umfassend über das Thema informieren und einen praxisorientierten Überblick geben.</p>
<p>Unter Self-Hosting wird der (auf unterschiedlichen Leveln) eigenständige Betrieb von Infrastruktur verstanden, auf der in diesem Fall LLMs und mit diesen zusammenhängende Software läuft.</p>
<h2 id="was-ist-ein-llm"><a class="header" href="#was-ist-ein-llm">Was ist ein LLM?</a></h2>
<p>LLMs (Large Language Models) sind große neuronale Sprachmodelle. Sie basieren auf neuronalen Netzwerken – mathematischen Strukturen, die aus vielen miteinander verbundenen Schichten bestehen. Diese Modelle werden durch die Anpassung von Millionen oder sogar Milliarden von Parametern trainiert, die zusammen mit der Modellarchitektur das fertige Modell definieren.</p>
<p>Die Textgenerierung erfolgt durch zahlreiche Matrizenrechnungen, die sich besonders effizient auf spezialisierter Hardware wie GPUs ausführen lassen. <strong>Open-Weight-Modelle</strong> sind als Dateien verfügbar, die sowohl die Architektur als auch die trainierten Parameter eines Modells enthalten – sie können daher selbst gehostet werden.</p>
<h2 id="warum-self-hosting"><a class="header" href="#warum-self-hosting">Warum Self-Hosting?</a></h2>
<p>Die Nutzung proprietärer LLM-Dienste ist mit  <strong>Abhängigkeiten</strong>, <strong>Intransparenz</strong> und mangelnder <strong>Kontrolle</strong> verbunden. Bei kommerziellen Anbietern bleibt unklar, wie Daten tatsächlich verarbeitet werden, wie die Modelle trainiert wurden und welche Eigenschaften sie besitzen. Auch die Funktionsweise eingebundener Tools wie Websuche sowie der tatsächliche Ressourcenverbrauch sind nicht nachvollziehbar. Self-Hosting adressiert diese Problematik und trägt zur <strong>digitalen Souveränität</strong> bei: Organisationen und Privatpersonen erlangen vollständige Transparenz und Kontrolle über alle Aspekte des LLM-Betriebs – von der Datenverarbeitung über die Modellauswahl bis hin zum Energie- und Ressourcenverbrauch.</p>
<h2 id="worauf-wird-selbst-gehostet"><a class="header" href="#worauf-wird-selbst-gehostet">Worauf wird selbst gehostet?</a></h2>
<p>Self-Hosting kann an verschiedenen Orten erfolgen:</p>
<ul>
<li><strong>Lokal (On-Premises)</strong>: Betrieb auf dem eigenen Laptop oder auf dedizierten Servern, die in den Räumlichkeiten einer Organisation oder Privatperson stehen. Dies bedeutet volle Kontrolle über das Setup, aber auch volle Verantwortung für Hardware und Wartung.</li>
<li><strong>Cloud</strong>: Server werden in verschiedenen Formen von Cloud-Providern zur Miete angeboten – entweder als virtualisierte Maschine (VM) oder als dedizierte physische Maschine (Bare Metal). Flexibel skalierbar, aber mit laufenden Kosten.</li>
<li><strong>Hybrid (Co-Hosting)</strong>: Kauf eigener Server, die jedoch in den Räumen eines Cloud-Providers betrieben und gewartet werden. Kombination aus Eigentum und professioneller Infrastruktur.</li>
</ul>
<h2 id="was-wird-selbst-gehostet"><a class="header" href="#was-wird-selbst-gehostet">Was wird selbst gehostet?</a></h2>
<p>Im Falle des Self-Hostings von LLM-Systemen stellt sich die Frage, welche Komponenten man selbst betreiben will. Hier lässt sich zwischen <strong>Frontend</strong> und <strong>Backend</strong> unterscheiden:</p>
<p><strong>Frontend</strong>: Meistens ein Chat Interface, über das Nutzer:innen mit dem LLM interagieren.</p>
<p><strong>Backend</strong>: Umfasst mehrere Komponenten:</p>
<ul>
<li><strong>Inference-Server</strong>: Software, auf der ein LLM betrieben wird. Auf solchen Servern können sowohl generative LLMs als auch Embedding-Modelle betrieben werden.</li>
<li><strong>Vektor-Datenbank</strong>: Wird für RAG (Retrieval-Augmented Generation) benötigt. RAG ergänzt informationsbezogene Anfragen an das LLM mit relevanten Dokumenten aus einem Suchschritt mithilfe einer Datenbank.</li>
<li><strong>API Gateway</strong>: Verwaltet und standardisiert den Zugriff auf die verschiedenen Backend-Dienste</li>
<li>Zusätzliche Dienste wie Authentifizierung und Dateipeicher</li>
</ul>
<p><strong>RAG im Detail</strong>: Bei RAG wird semantische Suche verwendet. Dabei verarbeitet ein Embedding-Modell Texte zu sogenannten Embeddings. Embeddings sind Vektore, die den Textinhalt in numerischer Form repräsentieren. Diese Embeddings ermöglichen es, inhaltlich ähnliche Dokumente zu finden und dem LLM als Kontext bereitzustellen.</p>
<p><strong>Fig. 1</strong> bietet einen graphischen Überblick über die Komponenten und deren Zusammenspiel:</p>
<pre class="mermaid">flowchart TB
    subgraph Frontend
        UI[Chat Interface]
    end

    subgraph Backend
        direction TB
        subgraph InferenceLayer[Inference Layer]
            LLMServer[Inference-Server für LLM]
            EmbedServer[Inference-Server für Embedding-Modell]
        end

        subgraph DataStorage[Datenschicht]
            VectorDB[(Vektor-Datenbank)]
        end

        subgraph Additional[Zusätzliche Komponenten]
            Tools[Tools]
            Gateway[API Gateway]
        end
    end

    UI --&gt;|Anfragen| Gateway
    Gateway --&gt;|Routen| LLMServer

    UI -.-&gt;|RAG Workflow| LLMServer
    LLMServer -.-&gt;|Suchanfrage| EmbedServer
    EmbedServer -.-&gt;|Embeddings| VectorDB
    VectorDB -.-&gt;|Relevante Dokumente| LLMServer

    Tools --&gt;|Erweiterte Funktionen| LLMServer
</pre>

<h2 id="aufbau-der-dokumentation"><a class="header" href="#aufbau-der-dokumentation">Aufbau der Dokumentation</a></h2>
<p>Die folgenden Kapitel vertiefen die hier eingeführten Konzepte systematisch:</p>
<p><strong><a href="#open-llms">Open LLMs</a></strong> erklärt den Unterschied zwischen Open-Weight- und Open-Source-Modellen, gibt einen Überblick über Lizenzmodelle und deren Implikationen, erläutert Quantisierung als zentrale Optimierungstechnik, stellt Hugging Face als primäre Bezugsquelle vor und beschreibt gängige Metriken und Benchmarks zur Modellbewertung.</p>
<p><strong><a href="#hosting-infrastruktur">Hosting-Infrastruktur</a></strong> vergleicht detailliert die verschiedenen Infrastrukturoptionen – von lokalen Servern über Cloud-Lösungen bis zu hybriden Ansätzen. Das Kapitel behandelt Hardware-Anforderungen (CPU vs. GPU, VRAM-Größen, GPU-Generationen) sowie Aspekte wie Energieeffizienz, Kühlungssysteme und langfristige Kostenbetrachtungen.</p>
<p><strong><a href="#hosting-software">Hosting-Software</a></strong> stellt konkrete Software-Komponenten für die verschiedenen Teile eines LLM-Systems vor – von Inference-Servern über Chat-Interfaces bis zu API-Gateways und Vektor-Datenbanken. Als praktisches Beispiel wird Parrotpark vorgestellt, ein vollständiges self-gehostetes LLM-System, das die Integration verschiedener Open-Source-Komponenten demonstriert.</p>
<p><strong><a href="#alternativen-und-vergleich">Alternativen</a></strong> diskutiert abschließend Alternativen zum vollständigen Self-Hosting, einschließlich hybrider Ansätze und kommerzieller Lösungen mit verschiedenen Compliance-Leveln.</p>
<hr>
<ol class="footnote-definition">
<li id="footnote-1">
<p>Die Slides des Vortrags lassen sich <a href="https://correlaid.github.io/datenfestival_ap3_session/">hier</a> finden, begleitender Code <a href="https://github.com/CorrelAid/datenfestival_ap3_session">hier</a>. <a href="#fr-1-1">↩</a></p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="open-llms"><a class="header" href="#open-llms">Open LLMs</a></h1>
<p>Für das Self-Hosting von LLMs stehen zahlreiche Modelle zur Verfügung. Dieses Kapitel gibt einen Überblick über die wichtigsten Aspekte bei der Auswahl und Bewertung von Modellen: von Lizenzfragen über technische Optimierungen bis hin zu Bezugsquellen und Bewertungsmetriken.</p>
<h2 id="open-vs-open-source"><a class="header" href="#open-vs-open-source">Open vs Open Source</a></h2>
<p>Im Kontext von Large Language Models wird häufig von “Open-Source-Modellen” gesprochen, obwohl die meisten dieser Modelle technisch gesehen <strong>Open-Weight-Modelle</strong> sind. Der Unterschied ist wichtig:</p>
<p><strong>Open-Weight-Modelle</strong> stellen die trainierten Modellparameter (Weights) öffentlich zur Verfügung. Nutzer:innen können diese Modelle herunterladen, ausführen und für eigene Zwecke verwenden. Allerdings sind oft weder die Trainingsdaten noch der vollständige Trainingscode verfügbar. Beispiele: Llama (Meta), Mistral (Mistral AI), Qwen (Alibaba).</p>
<p><strong>Open-Source-Modelle</strong> im engeren Sinne gehen weiter: Sie veröffentlichen nicht nur die Modellparameter, sondern auch die Trainingsdaten, den Trainingscode und die gesamte Entwicklungspipeline. Dies ermöglicht vollständige Reproduzierbarkeit und Transparenz. Beispiele: OLMo (Allen Institute for AI), Pythia (EleutherAI).</p>
<p>Für Self-Hosting sind beide Varianten geeignet, wobei Open-Weight-Modelle deutlich verbreiteter sind. Die Wahl zwischen beiden hängt davon ab, wie wichtig vollständige Transparenz und Reproduzierbarkeit für den jeweiligen Anwendungsfall sind.</p>
<h2 id="lizenzen"><a class="header" href="#lizenzen">Lizenzen</a></h2>
<p>Die Lizenzen von LLMs unterscheiden sich erheblich und haben direkten Einfluss darauf, wie die Modelle genutzt werden dürfen:</p>
<p><strong>Permissive Lizenzen</strong> (z.B. Apache 2.0, MIT): Erlauben kommerzielle Nutzung, Modifikation und Weiterverbreitung mit minimalen Einschränkungen. Beispiele: Mistral-Modelle, Qwen-Modelle.</p>
<p><strong>Restriktive Community-Lizenzen</strong>: Erlauben Nutzung und Modifikation, schränken aber kommerzielle Nutzung ein oder verlangen Lizenzgebühren ab einer bestimmten Nutzerzahl. Beispiel: Llama-Modelle haben eine Custom-Lizenz [@meta_llama4_policy], die bei über 700 Millionen monatlichen Nutzern eine separate Vereinbarung erfordert.</p>
<p><strong>Research-Only-Lizenzen</strong>: Beschränken die Nutzung auf nicht-kommerzielle Forschungszwecke. Solche Modelle eignen sich nicht für produktive Self-Hosting-Szenarien.</p>
<p>Manche Lizenzen unterscheiden sich auch je nach Land der Nutzer:in. So verbietet Meta die Nutzung von Llama in der EU.
Vor dem Einsatz eines Modells sollte die Lizenz sorgfältig geprüft werden, insbesondere bei kommerzieller Nutzung oder bei Organisationen mit vielen Nutzer:innen. Die Lizenzen sind in der Regel auf den Modell-Seiten bei Hugging Face oder auf den Websites der Entwickler dokumentiert.</p>
<h2 id="quantisierung"><a class="header" href="#quantisierung">Quantisierung</a></h2>
<p>Quantisierung ist eine Technik zur Reduzierung der Modellgröße und des Speicherbedarfs, die für Self-Hosting von zentraler Bedeutung ist.</p>
<p><strong>Funktionsweise</strong>: Bei der Quantisierung werden die Modellparameter von höherer Präzision (z.B. 16-Bit oder 32-Bit Floating Point) in niedrigere Präzision (z.B. 8-Bit, 4-Bit oder sogar 2-Bit) konvertiert. Dies reduziert den benötigten GPU-Speicher (VRAM) erheblich.</p>
<p><strong>Auswirkungen</strong>: Ein Modell mit 70 Milliarden Parametern (70B) benötigt in voller Präzision (FP16) etwa 140 GB VRAM. Mit 4-Bit-Quantisierung reduziert sich der Bedarf auf ca. 35 GB – ein Unterschied, der darüber entscheidet, ob ein Modell auf verfügbarer Hardware überhaupt lauffähig ist.</p>
<p><strong>Trade-offs</strong>: Quantisierung führt zu leichten Qualitätseinbußen bei den Modellantworten. Moderne Quantisierungsmethoden wie GPTQ, AWQ oder GGUF minimieren diese Verluste jedoch stark. In vielen Anwendungsfällen ist der Qualitätsunterschied zwischen 4-Bit und voller Präzision kaum wahrnehmbar, während die Geschwindigkeit deutlich zunimmt.</p>
<p><strong>Praktische Bedeutung</strong>: Für Self-Hosting sind quantisierte Modelle oft die einzige praktikable Option. Die meisten Inference-Server unterstützen verschiedene Quantisierungsformate, und auf Hugging Face sind bereits quantisierte Versionen beliebter Modelle verfügbar.</p>
<h2 id="hugging-face"><a class="header" href="#hugging-face">Hugging Face</a></h2>
<p><a href="https://huggingface.co/">Hugging Face</a> ist die zentrale Plattform für Open-Weight- und Open-Source-LLMs. Sie fungiert als Repository, Community-Hub und Entwicklungsplattform.</p>
<p><strong>Model Hub</strong>: Tausende von Modellen sind verfügbar, von kleinen spezialisierten Modellen bis zu großen Frontier-Modellen. Jedes Modell verfügt über eine Seite mit Beschreibung, Lizenzinformationen, Nutzungsbeispielen und Download-Links. Filter ermöglichen die Suche nach Modellgröße, Aufgabe, Lizenz und weiteren Kriterien.</p>
<p><strong>Praktische Nutzung</strong>: Modelle können direkt über die Webseite heruntergeladen oder programmgesteuert über die Hugging Face API bezogen werden. Die meisten Inference-Server können Modelle automatisch von Hugging Face laden, indem lediglich der Modellname angegeben wird (z.B. <code>mistralai/Mistral-7B-Instruct-v0.2</code>).</p>
<p><strong>Zusätzliche Ressourcen</strong>: Neben Modellen hostet Hugging Face auch Datasets und Spaces (interaktive Demos). Die Plattform bietet umfangreiche Dokumentation und eine aktive Community, die bei Fragen hilft.</p>
<p><strong>Alternativen</strong>: Während Hugging Face die dominante Plattform ist, gibt es Alternativen wie ModelScope (chinesischer Markt) oder direkte Downloads von Entwickler-Websites. Für die meisten Self-Hosting-Projekte ist Hugging Face jedoch die erste Anlaufstelle.</p>
<h2 id="metriken"><a class="header" href="#metriken">Metriken</a></h2>
<p>Die Bewertung von LLMs erfolgt anhand verschiedener Metriken und Benchmarks, die unterschiedliche Fähigkeiten testen:</p>
<p><strong>LLM-Leaderboards</strong>: Plattformen wie das <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a> von Hugging Face aggregieren Benchmark-Ergebnisse und ermöglichen den direkten Vergleich von Modellen. Diese Leaderboards werden kontinuierlich aktualisiert und sind hilfreich für eine erste Orientierung.</p>
<p><strong>Aufgabenbezogene Benchmarks</strong>: Verschiedene Benchmarks testen spezifische Fähigkeiten:</p>
<ul>
<li><strong>MMLU (Massive Multitask Language Understanding)</strong>: Testet Wissen aus 57 Themenbereichen (STEM, Geisteswissenschaften, Sozialwissenschaften)</li>
<li><strong>GSM8K</strong>: Mathematische Problemlösung (Grundschulmathematik)</li>
<li><strong>HumanEval</strong>: Programmierfähigkeiten (Code-Generierung)</li>
<li><strong>HellaSwag</strong>: Commonsense Reasoning</li>
<li><strong>TruthfulQA</strong>: Faktentreue und Resistenz gegen Fehlinformationen</li>
</ul>
<p><strong>Humanities-fokussierte Tests</strong>: Manche Benchmarks verwenden standardisierte Prüfungen aus verschiedenen Bildungssystemen, um die Modellleistung in geisteswissenschaftlichen Bereichen zu messen. Dies kann Examensaufgaben aus Literatur, Geschichte, Philosophie oder Ethik umfassen.</p>
<p><strong>Wichtige Einschränkungen</strong>: Benchmarks sind nützliche Orientierungshilfen, aber keine vollständige Bewertung. Sie können anfällig für Overfitting sein (Modelle werden auf Benchmark-Performance optimiert), bilden nicht alle relevanten Fähigkeiten ab (z.B. Kreativität, Dialogfähigkeit) und sagen wenig über das Verhalten in spezifischen Anwendungsfällen aus. Für Self-Hosting-Projekte empfiehlt sich, Modelle auch anhand eigener, anwendungsspezifischer Tests zu evaluieren.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hosting-infrastruktur"><a class="header" href="#hosting-infrastruktur">Hosting-Infrastruktur</a></h1>
<p>LLMs lassen sich prinzipiell auf fast allen Arten von Infrastruktur betreiben – theoretisch sogar auf einem Raspberry Pi. Die praktisch erreichbare Geschwindigkeit, Modellqualität und Länge des In- und Outputs unterscheiden sich jedoch erheblich je nach Hardware. Dieses Kapitel beschreibt die Hardware-Anforderungen für LLM-Hosting, den damit verbundenen Ressourcenverbrauch und die verschiedenen Infrastruktur-Optionen von lokal bis Cloud.</p>
<h2 id="hardware-anforderungen"><a class="header" href="#hardware-anforderungen">Hardware-Anforderungen</a></h2>
<h3 id="cpu-vs-gpu"><a class="header" href="#cpu-vs-gpu">CPU vs GPU</a></h3>
<p>Für das Hosting von LLMs stellt sich zunächst die grundlegende Frage: CPU oder GPU?</p>
<p><strong>CPU-Inferenz</strong> ist möglich und für sehr kleine Modelle oder Testszenarien praktikabel. Moderne CPUs können kleine quantisierte Modelle durchaus betreiben. Die Inferenz ist deutlich langsamer als auf GPUs, was für produktive Systeme mit Nutzerinteraktion meist inakzeptabel ist.</p>
<p><strong>GPU-Inferenz</strong> ist für Self-Hosting von LLMs praktisch der Standard. GPUs sind für die massiv parallelen Matrizenoperationen optimiert, die bei der Textgenerierung anfallen, wewegen die Geschwindigkeitsunterschiede immens sind.</p>
<p><strong>Marktlage</strong>: Der GPU-Markt für LLM-Inferenz wird dominiert von <strong>NVIDIA</strong>, deren CUDA-Ökosystem den De-facto-Standard darstellt. Die meisten Inference-Server und ML-Frameworks sind primär für NVIDIA-GPUs optimiert. <strong>AMD</strong> holt jedoch auf: ROCm (AMDs Alternative zu CUDA) wird zunehmend besser unterstützt, und AMD-GPUs bieten oft ein besseres Preis-Leistungs-Verhältnis. Für kritische Produktivsysteme ist NVIDIA derzeit die sicherere Wahl, für experimentelle Setups kann AMD eine kostengünstige Alternative sein.</p>
<h3 id="gpu-spezifikationen"><a class="header" href="#gpu-spezifikationen">GPU-Spezifikationen</a></h3>
<p>Bei der Auswahl von GPUs für LLM-Hosting sind mehrere Faktoren entscheidend:</p>
<p><strong>VRAM-Größe</strong>: Der GPU-Speicher (VRAM) ist oft der limitierende Faktor. Ein Modell muss vollständig in den VRAM passen, um effizient zu laufen</p>
<p>Gängige GPU-VRAM-Größen:</p>
<ul>
<li>Consumer-GPUs (z.B. RTX 4090): 24 GB</li>
<li>Professional GPUs (z.B. A100): 40 GB oder 80 GB</li>
<li>High-End Datacenter GPUs (z.B. H100, H200): 80 GB bzw. 141 GB (H200)</li>
</ul>
<p>Die H200 mit 141 GB VRAM stellt aktuell das obere Ende des Spektrums dar und ermöglicht den Betrieb sehr großer Modelle auf einer einzelnen GPU.</p>
<p><strong>GPU-Generationen</strong>: Neuere GPU-Generationen unterstützen moderne Datentypen und Quantisierungstechniken besser als ältere Generationen. Die Unterstützung für mache Datentypen in neueren Generationen eingeführt. Bei der Hardwareauswahl sollte daher nicht nur die VRAM-Größe, sondern auch die GPU-Generation berücksichtigt werden, da dies direkten Einfluss auf verfügbare Quantisierungsoptionen und Performance hat.</p>
<h2 id="ressourcenverbrauch"><a class="header" href="#ressourcenverbrauch">Ressourcenverbrauch</a></h2>
<p>Der Betrieb von LLM-Infrastruktur ist ressourcenintensiv. Die beiden Hauptfaktoren sind Kühlung und Stromverbrauch, die eng miteinander verbunden sind.</p>
<h3 id="kühlung"><a class="header" href="#kühlung">Kühlung</a></h3>
<p>Leistungsfähige GPUs erzeugen erhebliche Abwärme, die abgeführt werden muss, um Überhitzung und Hardware-Schäden zu verhindern. Die Wahl des Kühlungssystems hat direkten Einfluss auf Energieeffizienz und Umweltauswirkungen.</p>
<p><strong>Luftkühlung</strong> ist der Standard bei den meisten Cloud-Providern und für lokales Hosting. Sie ist technisch einfach und zuverlässig: Ventilatoren transportieren kühle Luft an die Hardware und führen erwärmte Luft ab. Der Nachteil: Luftkühlung ist energieintensiv, da große Luftmengen bewegt werden müssen und die Kühleffizienz begrenzt ist. In Rechenzentren muss zusätzlich die Raumtemperatur reguliert werden, was den Energieaufwand weiter erhöht. Anbieter wie Hetzner setzen ausschließlich auf Luftkühlung.</p>
<p><strong>Wasserkühlung</strong> (bzw. Flüssigkeitskühlung) ist deutlich effizienter: Wasser kann Wärme  besser transportieren als Luft bei gleichem Volumen. Moderne Flüssigkeitskühlsysteme können Abwärme direkt am Chip abführen und reduzieren den Energieaufwand für Kühlung erheblich . Allerdings ist Wasserkühlung technisch aufwändiger und teurer in der Installation. Zudem verdunstet ein Teil des Kühlwassers, was zu relevantem Wasserverbrauch führt</p>
<p><strong>Transparenz und Nachhaltigkeit</strong>: Die Umweltauswirkungen der Kühlsysteme sind oft schwer nachvollziehbar. Life Cycle Assessments (LCAs), wie sie beispielsweise Mistral AI veröffentlicht hat, bieten einen systematischen Ansatz zur Bewertung von Energieverbrauch und Umweltauswirkungen über den gesamten Lebenszyklus – von der Hardware-Produktion über den Betrieb bis zur Entsorgung.</p>
<h3 id="strom"><a class="header" href="#strom">Strom</a></h3>
<p>CPU, GPU und andere Teile eines Servers, wie das Kühlungssystem, verbrauchen Strom. Besonders hoch ist der Verbrauch bei GPUs. Datacenter-GPUs haben einen erheblichen Energiebedarf, der je nach Modell und Last zwischen einigen hundert Watt bis über 700 Watt liegen kann. Der Gesamtstromverbrauch eines Systems liegt deutlich höher, da CPU, RAM, Netzwerk und Kühlung hinzukommen.</p>
<p>Bei lokalem Hosting ist der Strompreis langfristig oft der Haupt-Kostenpunkt. Bei Cloud-Hosting sind diese Stromkosten in den Mietpreisen enthalten.</p>
<h2 id="infrastruktur-optionen"><a class="header" href="#infrastruktur-optionen">Infrastruktur-Optionen</a></h2>
<p>Nach der Klärung der Hardware-Anforderungen und des Ressourcenverbrauchs stellt sich die Frage: Wo wird die Infrastruktur betrieben? Es gibt grundsätzlich drei Optionen: lokal, Cloud oder hybrid.</p>
<h3 id="lokal-on-premises"><a class="header" href="#lokal-on-premises">Lokal (On-Premises)</a></h3>
<p>Bei lokalem Hosting wird die Hardware selbst betrieben – auf dem eigenen PC/Laptop, einem Home-Server oder in einem eigenen Rechenzentrum. Der eigenet PC oder Laptop hat jedoch keine mit in der Cloud genutzen vergleichbaren GPU</p>
<h3 id="cloud"><a class="header" href="#cloud">Cloud</a></h3>
<p>Cloud-Hosting bietet flexible Mietoptionen für Server bei externen Anbietern. Es lassen sich drei Modelle unterscheiden:</p>
<p><strong>Dedicated Server</strong>: Miete dedizierter Server mit GPUs (monatlich oder stündlich). Der Server läuft kontinuierlich. Anbieter wie Hetzner oder Scaleway bieten solche Optionen in der EU an.</p>
<p><strong>Serverless / On-Demand</strong>: Server werden nur bei Bedarf gestartet und Abrechnung erfolgt pro Minute. Ideal für sporadische Nutzung. Anbieter wie Modal oder Beam Cloud spezialisieren sich auf ML-Workloads.</p>
<p><strong>Co-Location</strong>: Eigene Hardware wird in einem Rechenzentrum eines Providers betrieben, der Strom, Kühlung und Netzwerk bereitstellt. Kombination aus Eigentum und professioneller Infrastruktur.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hosting-software"><a class="header" href="#hosting-software">Hosting-Software</a></h1>
<ul>
<li>many active open source solutions</li>
</ul>
<p><strong>Fig. 1</strong> zeigt ein konkretes Beispiel: Parrotpark<sup class="footnote-reference" id="fr-1-1-1"><a href="#footnote-1-1">1</a></sup>, ein komplett self-gehostetes LLM-System. Die Infrastruktur besteht aus zwei Servern:</p>
<ul>
<li><strong>Eingangsserver</strong>: Permanenter Server, der Proxy, Datenbanken und einen Scheduler hostet</li>
<li><strong>Ephemerer GPU Server</strong>: Wird vom Scheduler regelmäßig neu erstellt und hostet die rechenintensiven LLM-Komponenten</li>
</ul>
<p>Während nicht jedes Self-Hosting Setup so komplex sein muss, zeigt dieses Beispiel die verschiedenen Komponenten einer Komplettlösung.</p>
<p><strong>Konkrete Software-Komponenten in Parrotpark</strong>:</p>
<ul>
<li><strong>Chat Interface</strong>: <a href="https://www.librechat.ai/">LibreChat</a></li>
<li><strong>Inference-Server</strong>: <a href="https://docs.vllm.ai/en/latest/">vLLM</a> (für generatives LLM und Embedding-Modell)</li>
<li><strong>Vektor-Datenbank</strong>: <a href="https://github.com/pgvector/pgvector">pgvector</a></li>
<li><strong>API Gateway</strong>: <a href="https://www.litellm.ai/">LiteLLM</a> – ermöglicht mehr Kontrolle (z.B. Token-Verwaltung) und standardisiert die API</li>
<li><strong>Authentifizierung</strong>: Keycloak für User-Verwaltung und SSO (Single Sign-On)</li>
<li><strong>Anwendungsdatenbank</strong>: Speichert Chatverläufe und Nutzereinstellungen für LibreChat und LiteLLM</li>
<li><strong>Proxy</strong>: Caddy – regelt SSL-Verschlüsselung für HTTPS und leitet Anfragen weiter</li>
<li>S3-Buckets für Dateispeicherung, Scheduler für automatische Server-Verwaltung</li>
</ul>
<pre class="mermaid">flowchart LR
    subgraph proxy[Eingangsserver]
    direction TB
      Caddy
      Databases[Vektor-DB und Anwendungs-DB]
      Scheduler
    end
    subgraph gpu[Ephemeral GPU Server]
    direction TB
      vLLM[vLLM Inference-Server]
      LiteLLM[LiteLLM API Gateway]
      LibreChat[LibreChat Chat Interface]
    end
    LibreChat --&gt;|SSO Auth| Keycloak
    buckets[S3 Buckets Tools]
    LibreChat --&gt;|Speicherung| buckets
    LiteLLM --- Databases
    LibreChat --- Databases
    Caddy --&gt;|Proxy| LiteLLM
    Caddy --&gt;|Proxy| LibreChat
    Scheduler --&gt;|Erstellt periodisch| gpu
</pre>

<hr>
<ol class="footnote-definition">
<li id="footnote-1-1">
<p>Mehr Informationen zu Parrotpark <a href="https://github.com/CorrelAid/parrotpark">hier</a>. <a href="#fr-1-1-1">↩</a></p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="alternativen-und-vergleich"><a class="header" href="#alternativen-und-vergleich">Alternativen und Vergleich</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="über-das-material"><a class="header" href="#über-das-material">Über das Material</a></h1>
<p>Die Inhalte dieser Seite wurden von Jonas Stettner (<a href="https://correlaid.org">CorrelAid e.V.</a>, tätig im <a href="https://civic-data.de">Civic Data Lab</a>) erstellt.</p>
<p>Diese Seite sowie Code und Dateien der Experimente finden sich in einem <a href="https://github.com/CorrelAid/cdl_llm_hosting_material">GitHub Repository</a>.</p>
<h2 id="lizenz"><a class="header" href="#lizenz">Lizenz</a></h2>
<p><a href="https://creativecommons.org/licenses/by/4.0/deed.de"><img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg" alt="License: CC BY 4.0"></a></p>
<ul>
<li>Attributionsvorschlag: <em>”<a href="https://correlaid.github.io/cdl_llm_hosting_material/">LLM-Hosting</a>” von Jonas Stettner (CorrelAid e.V. / Civic Data Lab), abgerufen am <strong>DATUM</strong>. Lizensiert unter <a href="https://creativecommons.org/licenses/by/4.0/deed.de">CC-BY 4.0</a>.</em></li>
<li>Attributionsvorschlag für einzelne Abschnitte / Unterseiten: <em>”<strong>[Unterseite]</strong>” in ”<a href="https://correlaid.github.io/cdl_llm_hosting_material/">LLM-Hosting</a>” von Jonas Stettner (CorrelAid e.V. / Civic Data Lab), abgerufen am [Datum]. Lizensiert unter <a href="https://creativecommons.org/licenses/by/4.0/deed.de">CC-BY 4.0</a>.</em></li>
</ul>
<h2 id="beitragen"><a class="header" href="#beitragen">Beitragen</a></h2>
<p>Du möchtest etwas beitragen zu dieser Website? Darüber freuen wir uns sehr!</p>
<p>Wenn du dich mit Git, GitHub und Markdown auskennst, freuen wir uns über einen Issue und dazugehörigen Pull Request im <a href="https://github.com/CorrelAid/cdl_llm_hosting_material">GitHub Repository</a>. Du hattest beim Lesen des vorherigen Satzes nur Fragezeichen und innere “hä?” Momente? Gar kein Problem! Schreib uns gern eine Email an <a href="mailto:leo.p@correlaid.org">jonas.s@correlaid.org</a> mit deinen Erkenntnissen. Wir kümmern uns dann um die Online-Stellung!</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid-eefea253.min.js"></script>
        <script src="mermaid-init-ccf746f1.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
